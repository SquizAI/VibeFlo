import React, { useState, useEffect, useRef, useCallback } from 'react';
import { Note as NoteComponent } from './components/Note';
import { DndProvider } from 'react-dnd';
import { HTML5Backend } from 'react-dnd-html5-backend';
import { Routes, Route, BrowserRouter, Link, useNavigate } from 'react-router-dom';
import { Note, Task, Position, Size } from './types';
import { Mic, Home, Grid, Move, Minimize, Zap, Search, Command, HelpCircle, FileStack, ZoomIn, Settings, Plus, X, ChevronLeft, ChevronRight } from 'lucide-react';
import './styles.css';
import './styles/taskListStyles.css';
import './styles/dictationPreview.css';
import AgentSidebar from './components/AgentSidebar';
import CommandPalette from './components/CommandPalette';
import AgentFeedback, { AgentActivityType } from './components/AgentFeedback';
import NoteSidebar from './components/NoteSidebar';
import DictationPreview from './components/DictationPreview';
import { transcribeAudio, enhanceTranscribedText, extractKeyTerms, createFallbackNoteFromDictation } from './lib/ai';
import { openai, validateApiKey } from './lib/openai';

// Mock nutrition database for common grocery items
const NUTRITION_DATABASE: { [key: string]: { calories: number, protein: number, carbs: number, fat: number } } = {
  "apple": { calories: 95, protein: 0.5, carbs: 25, fat: 0.3 },
  "banana": { calories: 105, protein: 1.3, carbs: 27, fat: 0.4 },
  "orange": { calories: 62, protein: 1.2, carbs: 15, fat: 0.2 },
  "milk": { calories: 103, protein: 8, carbs: 12, fat: 2.4 },
  "bread": { calories: 265, protein: 9, carbs: 49, fat: 3.2 },
  "chicken": { calories: 165, protein: 31, carbs: 0, fat: 3.6 },
  "beef": { calories: 250, protein: 26, carbs: 0, fat: 17 },
  "rice": { calories: 130, protein: 2.7, carbs: 28, fat: 0.3 },
  "pasta": { calories: 131, protein: 5, carbs: 25, fat: 1.1 },
  "cheese": { calories: 113, protein: 7, carbs: 0.4, fat: 9 },
  "eggs": { calories: 78, protein: 6, carbs: 0.6, fat: 5 },
  "salmon": { calories: 208, protein: 20, carbs: 0, fat: 13 },
  "tomato": { calories: 18, protein: 0.9, carbs: 3.9, fat: 0.2 },
  "potato": { calories: 77, protein: 2, carbs: 17, fat: 0.1 },
  "onion": { calories: 44, protein: 1.2, carbs: 10, fat: 0.1 },
  "carrot": { calories: 41, protein: 0.9, carbs: 10, fat: 0.2 },
  "broccoli": { calories: 55, protein: 3.7, carbs: 11, fat: 0.6 },
  "spinach": { calories: 23, protein: 2.9, carbs: 3.6, fat: 0.4 },
  "avocado": { calories: 160, protein: 2, carbs: 8.5, fat: 14.7 },
  "olive oil": { calories: 119, protein: 0, carbs: 0, fat: 13.5 },
  "almonds": { calories: 164, protein: 6, carbs: 6, fat: 14 },
  "oats": { calories: 68, protein: 2.4, carbs: 12, fat: 1.4 },
  "yogurt": { calories: 59, protein: 3.5, carbs: 5, fat: 3.1 },
  "sugar": { calories: 16, protein: 0, carbs: 4, fat: 0 },
  "flour": { calories: 364, protein: 10, carbs: 76, fat: 1 },
  "butter": { calories: 102, protein: 0.1, carbs: 0, fat: 11.5 },
  "chocolate": { calories: 155, protein: 2.1, carbs: 14, fat: 10 }
};

// Define an extended NoteInsights type
interface ExtendedNoteInsights {
  key_topics?: string[];
  timeframe?: string;
  action_required?: boolean;
  summary?: string;
  nutrition_summary?: {
    calories: number;
    protein: number;
    carbs: number;
    fat: number;
  };
}

// Define interfaces for the task and item extensions
interface NutritionData {
  calories: number;
  protein: number;
  carbs: number;
  fat: number;
}

interface ExtendedTask {
  id: string;
  text: string;
  done: boolean;
  subtasks: any[];
  isExpanded: boolean;
  depth: number;
  category: string;
  quantity?: string;
  nutrition?: NutritionData;
}

function App() {
  // State for notes with empty initial state (no demo notes)
  const [notes, setNotes] = useState<Note[]>([]);
  const [canvasOffset, setCanvasOffset] = useState({ x: 0, y: 0 });
  const [isDragging, setIsDragging] = useState(false);
  const [startPoint, setStartPoint] = useState({ x: 0, y: 0 });
  const [scale, setScale] = useState(1);
  const [showMiniMap, setShowMiniMap] = useState(false);
  const [canvasSize, setCanvasSize] = useState({ width: 5000, height: 5000 });
  const [viewportSize, setViewportSize] = useState({ width: 0, height: 0 });
  const [selectedNoteId, setSelectedNoteId] = useState<string | null>(null);
  const [isSpacePressed, setIsSpacePressed] = useState(false);
  const [lastMousePosition, setLastMousePosition] = useState({ x: 0, y: 0 });
  const canvasRef = useRef<HTMLDivElement>(null);
  const viewportRef = useRef<HTMLDivElement>(null);
  const [isRecording, setIsRecording] = useState(false);
  const [recordingTime, setRecordingTime] = useState(0);
  const [recordingInterval, setRecordingInterval] = useState<NodeJS.Timeout | null>(null);
  const [audioChunks, setAudioChunks] = useState<BlobPart[]>([]);
  const [processingStatus, setProcessingStatus] = useState('');
  const [isProcessing, setIsProcessing] = useState(false);
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const mediaStreamRef = useRef<MediaStream | null>(null);
  
  // Agent Sidebar state
  const [showAgentSidebar, setShowAgentSidebar] = useState(false);
  const [showCommandPalette, setShowCommandPalette] = useState(false);
  
  // Voice dictation preview state
  const [showDictationPreview, setShowDictationPreview] = useState(false);
  const [dictationNotes, setDictationNotes] = useState<any[]>([]);
  
  // Agent Feedback component state
  const [showAgentFeedback, setShowAgentFeedback] = useState(false);
  const [agentActivity, setAgentActivity] = useState<AgentActivityType>("idle");
  const [agentMessage, setAgentMessage] = useState("");
  const [agentProgress, setAgentProgress] = useState(0);
  const [agentConfidence, setAgentConfidence] = useState(0);
  
  // Notes Sidebar state
  const [showNoteSidebar, setShowNoteSidebar] = useState(false);
  const [showMovementTooltip, setShowMovementTooltip] = useState(true);
  const [focusedNoteId, setFocusedNoteId] = useState<string | null>(null);
  
  // Voice dictation state
  const [visibleTranscript, setVisibleTranscript] = useState('');
  const [showDictationPopup, setShowDictationPopup] = useState(false);

  const [isDraggingCanvas, setIsDraggingCanvas] = useState(false);
  const [clickedOnNote, setClickedOnNote] = useState(false);

  const [recording, setRecording] = useState(false);
  const [transcript, setTranscript] = useState('');
  const [interimTranscript, setInterimTranscript] = useState('');

  // Toggle mini-map visibility
  const toggleMiniMap = () => {
    setShowMiniMap(!showMiniMap);
  };

  function handleMove(id: string, newPosition: Position) {
    // When moving notes, we need to account for current canvas offset
    updateNote(id, { position: newPosition });
  }

  function addNote() {
    const newNote: Note = {
      id: String(Date.now()),
      type: 'sticky',
      content: 'New note',
      color: 'yellow',
      position: {
        x: -canvasOffset.x + window.innerWidth / 2 - 150,
        y: -canvasOffset.y + window.innerHeight / 2 - 150
      }
    };

    setNotes(prev => [...prev, newNote]);
  }

  function updateNote(id: string, updates: Partial<Note>) {
    setNotes(prev => prev.map(note => {
      if (note.id === id) {
        return { ...note, ...updates };
      }
      return note;
    }));
  }

  function removeNote(id: string) {
    setNotes(prev => prev.filter(note => note.id !== id));
  }

  function formatRecordingTime(milliseconds: number) {
    const seconds = Math.floor(milliseconds / 1000);
    const minutes = Math.floor(seconds / 60);
    const remainingSeconds = seconds % 60;
    return `${minutes}:${remainingSeconds < 10 ? '0' : ''}${remainingSeconds}`;
  }

  // Toggle Agent Sidebar
  const toggleAgentSidebar = () => {
    setShowAgentSidebar(!showAgentSidebar);
  };
  
  // Command Palette functions
  const openCommandPalette = () => {
    setShowCommandPalette(true);
  };
  
  const closeCommandPalette = () => {
    setShowCommandPalette(false);
  };
  
  // Add function to toggle notes sidebar
  const toggleNoteSidebar = () => {
    setShowNoteSidebar(!showNoteSidebar);
  };
  
  // Function to focus on a specific note (center it in viewport)
  const focusOnNote = (noteId: string) => {
    const noteToFocus = notes.find(n => n.id === noteId);
    if (noteToFocus && viewportRef.current) {
      // Center the note in the viewport
      const viewportWidth = viewportRef.current.clientWidth;
      const viewportHeight = viewportRef.current.clientHeight;
      
      const newOffset = {
        x: -(noteToFocus.position.x - viewportWidth / 2 + 150), // 150 is approx half of note width
        y: -(noteToFocus.position.y - viewportHeight / 2 + 150) // 150 is approx half of note height
      };
      
      setCanvasOffset(newOffset);
      
      // Temporarily highlight the note
      setFocusedNoteId(noteId);
      setTimeout(() => {
        setFocusedNoteId(null);
      }, 2000);
    }
  };
  
  // Function to simulate agent activity
  const simulateAgentActivity = (activity: AgentActivityType, message: string, duration: number = 3000) => {
    setShowAgentFeedback(true);
    setAgentActivity(activity);
    setAgentMessage(message);
    setAgentConfidence(0);
    setAgentProgress(0);
    
    // Simulate progress
    const interval = 100;
    const steps = duration / interval;
    let currentStep = 0;
    
    const progressInterval = setInterval(() => {
      currentStep++;
      setAgentProgress(Math.min(100, (currentStep / steps) * 100));
      
      if (currentStep >= steps) {
        clearInterval(progressInterval);
        
        // Set confidence based on activity type
        if (activity === "searching" || activity === "generating") {
          setAgentConfidence(Math.random() * 30 + 70); // 70-100% confidence
        }
        
        // Auto hide after completion for certain activities
        if (activity !== "error" && activity !== "waiting") {
          setTimeout(() => {
            setShowAgentFeedback(false);
          }, 2000);
        }
      }
    }, interval);
  };
  
  // Function to handle the dictation process
  const toggleRecording = async () => {
    if (recording) {
      // Stop recording
      setRecording(false);
      
      // Process the final transcript
      if (transcript) {
        await processDictationTranscript(transcript);
      }
    } else {
      // Start new recording
      setRecording(true);
      setTranscript('');
      setInterimTranscript('');
      setDictationNotes([]);
      setShowDictationPreview(true);
      
      try {
        const recognition = new (window as any).webkitSpeechRecognition();
        recognition.continuous = true;
        recognition.interimResults = true;
        
        let finalTranscript = '';
        
        recognition.onresult = (event: any) => {
          let interimTranscriptValue = '';
          
          for (let i = event.resultIndex; i < event.results.length; i++) {
            const transcript = event.results[i][0].transcript;
            if (event.results[i].isFinal) {
              finalTranscript += transcript + ' ';
            } else {
              interimTranscriptValue += transcript;
            }
          }
          
          setTranscript(finalTranscript);
          setInterimTranscript(interimTranscriptValue);
        };
        
        recognition.onerror = (event: any) => {
          console.error('Speech recognition error', event.error);
          setRecording(false);
        };
        
        recognition.onend = () => {
          if (recording) {
            setRecording(false);
          }
        };
        
        recognition.start();
        
        // Save the recognition object to potentially stop it later
        (window as any).currentRecognition = recognition;
      } catch (error) {
        console.error('Speech recognition failed:', error);
        setRecording(false);
      }
    }
  };
  
  // Process the dictation transcript by calling OpenAI API
  const processDictationTranscript = async (finalTranscript: string) => {
    try {
      console.log("Processing dictation transcript...");
      setProcessingStatus("Creating notes from dictation...");

      // Validate the API key first
      const keyValidation = validateApiKey();
      if (!keyValidation.valid) {
        console.error(`OpenAI API key validation failed: ${keyValidation.message}`);
        
        // Special handling for project API keys - they need a different approach
        const apiKey = import.meta.env.VITE_OPENAI_API_KEY;
        const isProjectKey = apiKey && typeof apiKey === 'string' && apiKey.startsWith('sk-proj-');
        
        if (isProjectKey) {
          console.log("Detected project API key - using OpenAI client library instead of fetch");
          
          try {
            // Use the client library which handles project API keys correctly
            const completion = await openai.chat.completions.create({
              model: 'gpt-4o',
              messages: [
                {
                  role: 'system',
                  content: `You are an AI that analyzes dictated text to create multiple detailed notes.

ALWAYS create SEPARATE notes by categorizing content aggressively:
1. ANY mention of food items, ingredients, recipes â†’ Create a dedicated GROCERY_LIST note
2. ANY mention of tasks, todos, deadlines â†’ Create a dedicated TASK_LIST note
3. ANY other information â†’ Create a GENERAL_NOTE

CRITICAL RULES:
- CREATE MULTIPLE NOTES - don't combine different types of content 
- For grocery lists, extract ALL food items mentioned and categorize them
- For recipe notes, create a dedicated note for EACH recipe mentioned
- Be extremely aggressive in separating content - when in doubt, create more notes
- NEVER combine grocery items with tasks - they MUST be separate notes
- Return detailed JSON that can create multiple notes from one dictation

Return a JSON array of notes where each note has:
- "type": "grocery_list", "task_list", "recipe", or "general_note"
- "title": descriptive title
- "content": main content text
- "items": array of structured items with text, category, and completed fields`
                },
                { role: 'user', content: finalTranscript }
              ],
              temperature: 0.2,
              response_format: { type: "json_object" }
            });
            
            // Extract the content - this will be a string containing JSON
            const jsonContent = completion.choices[0].message.content || "";
            console.log("OpenAI client lib response content:", jsonContent);
            
            // Parse the JSON from the content
            try {
              // Clean any markdown code block formatting that GPT-4o might add
              // even with response_format, sometimes it still includes formatting
              const cleanJsonContent = jsonContent
                .replace(/^```json\s*/g, '')  // Remove opening ```json
                .replace(/\s*```$/g, '')      // Remove closing ```
                .trim();
                
              let parsedResult;
              try {
                parsedResult = JSON.parse(cleanJsonContent);
              } catch (jsonError) {
                console.log("Error parsing cleaned content, trying original content");
                parsedResult = JSON.parse(jsonContent);
              }
              
              const result = parsedResult;
              console.log("Parsed result:", result);
              
              // Create notes based on the response
              const notes: Note[] = [];
              
              // Check if result is an array of notes or a single note
              if (Array.isArray(result.notes)) {
                console.log("Processing array of notes:", result.notes.length);
                
                // Process each note in the array
                result.notes.forEach((noteData: any, index: number) => {
                  const note = createNoteFromData(noteData, index * 50);
                  if (note) notes.push(note);
                });
              } else {
                // Fallback to previous behavior for backward compatibility
                const note = createNoteFromData(result, 0);
                if (note) notes.push(note);
              }
              
              console.log(`Created ${notes.length} notes from dictation`);
              setDictationNotes(notes);
              setShowDictationPreview(true);
              
              // Add debug message to track visibility
              console.log(`ðŸ”” DICTATION PREVIEW SHOULD NOW BE VISIBLE - Notes count: ${notes.length}`);
              
              // Add a small delay to ensure state is updated before continuing
              setTimeout(() => {
                document.body.style.overflow = 'hidden'; // Force body to not scroll
                const previewDiv = document.querySelector('.dictation-preview-panel');
                if (previewDiv) {
                  (previewDiv as HTMLElement).style.display = 'block';
                  (previewDiv as HTMLElement).style.opacity = '1';
                  console.log('ðŸ” Found and forced dictation preview to be visible');
                } else {
                  console.warn('âš ï¸ Could not find dictation preview element');
                }
              }, 100);
              
              return;
            } catch (jsonError) {
              console.error("Error parsing JSON from completion:", jsonError);
              createFallbackNote(finalTranscript, "Error parsing AI response");
              return;
            }
          } catch (openaiError) {
            console.error("OpenAI client library error:", openaiError);
            createFallbackNote(finalTranscript, "OpenAI API error");
            return;
          }
        } else {
          // Create a fallback note for non-project API key errors
          createFallbackNote(transcript, `API key error: ${keyValidation.message}`);
          return;
        }
      }

      try {
        const response = await fetch('https://api.openai.com/v1/chat/completions', {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
            'Authorization': `Bearer ${import.meta.env.VITE_OPENAI_API_KEY}`
          },
          body: JSON.stringify({
            model: 'gpt-4o-2024-08-06',
            messages: [
              {
                role: 'system',
                content: `Analyze this dictation and categorize it. Be VERY aggressive in identifying structure, even if not explicitly formatted.
                
IF IT CONTAINS ANY food items, ingredients, household products â†’ ALWAYS classify as grocery_list
IF IT CONTAINS ANY actions, tasks, todos, deadlines â†’ ALWAYS classify as task_list
OTHERWISE â†’ classify as general_note

IMPORTANT RULES:
- Default to structured lists (task_list or grocery_list) whenever possible
- For grocery_list, extract ALL food items and categorize them (produce, dairy, meat, etc.)
- For task_list, identify ALL actions and mark with appropriate priorities
- ALWAYS extract items as structured data, even from general notes
- ALWAYS include a descriptive title`
              },
              { role: 'user', content: transcript }
            ],
            response_format: {
              type: 'json_schema',
              json_schema: {
                name: 'dictation_analysis',
                schema: {
                  type: 'object',
                  properties: {
                    type: {
                      type: 'string',
                      enum: ['grocery_list', 'task_list', 'general_note']
                    },
                    title: { type: 'string' },
                    content: { type: 'string' },
                    items: {
                      type: 'array',
                      items: {
                        type: 'object',
                        properties: {
                          text: { type: 'string' },
                          category: { type: 'string' },
                          completed: { type: 'boolean' },
                          priority: { type: 'string', enum: ['high', 'medium', 'low'] },
                          due_date: { type: 'string' },
                          quantity: { type: 'string' }
                        },
                        required: ['text', 'category', 'completed'],
                        additionalProperties: false
                      }
                    },
                    insights: {
                      type: 'object',
                      properties: {
                        key_topics: { 
                          type: 'array', 
                          items: { type: 'string' } 
                        },
                        timeframe: { type: 'string' },
                        action_required: { type: 'boolean' },
                        summary: { type: 'string' }
                      }
                    }
                  },
                  required: ['type', 'title', 'content', 'items'],
                  additionalProperties: false
                },
                strict: true
              }
            }
          })
        });

        console.log("OpenAI response:", response);

        if (!response.ok) {
          throw new Error(`OpenAI API error: ${response.status} ${response.statusText}`);
        }

        const result = await response.json();
        console.log("Parsed result:", result);

        // Create notes based on the response
        const notes: Note[] = [];
        
        // Create the appropriate note type based on the analysis
        if (result.type === 'grocery_list') {
          const groceryNote: Note = {
            id: `note-${Date.now()}`,
            type: 'task',
            color: 'green',
            content: `ðŸ“ ${result.title}\n\nðŸ›’ Grocery List:`,
            position: { x: 100, y: 100 },
            size: { width: 250, height: 300 },
            tasks: result.items.map((item: any) => ({
              id: `task-${Date.now()}-${Math.random().toString(36).substring(2, 9)}`,
              text: item.text + (item.quantity ? ` (${item.quantity})` : ''),
              done: item.completed,
              subtasks: [],
              isExpanded: false,
              depth: 0,
              category: item.category || 'grocery'
            })),
            insights: result.insights || {
              key_topics: ['groceries', 'shopping'],
              action_required: true,
              timeframe: 'soon'
            }
          };
          notes.push(groceryNote);
        } 
        else if (result.type === 'task_list') {
          const taskNote: Note = {
            id: `note-${Date.now()}`,
            type: 'task',
            color: 'blue',
            content: `ðŸ“ ${result.title}\n\nâœ… Task List:`,
            position: { x: 100, y: 100 },
            size: { width: 250, height: 300 },
            tasks: result.items.map((item: any) => ({
              id: `task-${Date.now()}-${Math.random().toString(36).substring(2, 9)}`,
              text: `${item.priority === 'high' ? 'âš ï¸ ' : ''}${item.text}${item.due_date ? ` (Due: ${item.due_date})` : ''}`,
              done: item.completed,
              subtasks: [],
              isExpanded: false,
              depth: 0,
              category: item.category || 'task'
            })),
            insights: result.insights || {
              key_topics: ['tasks', 'todos'],
              action_required: true,
              timeframe: 'upcoming'
            }
          };
          notes.push(taskNote);
        }
        else {
          // Default to general note
          const regularNote: Note = {
            id: `note-${Date.now()}`,
            type: 'sticky',
            color: 'yellow',
            content: result.content,
            position: { x: 100, y: 100 },
            size: { width: 200, height: 200 },
            insights: result.insights
          };
          notes.push(regularNote);
          
          // If there are items in a general note, also create a task list
          if (result.items && result.items.length > 0) {
            const extractedTaskNote: Note = {
              id: `note-extract-${Date.now()}`,
              type: 'task',
              color: 'purple',
              content: `ðŸ“ Extracted Points from: ${result.title}`,
              position: { x: 100, y: 350 },
              size: { width: 250, height: 300 },
              tasks: result.items.map((item: any) => ({
                id: `task-${Date.now()}-${Math.random().toString(36).substring(2, 9)}`,
                text: item.text,
                done: item.completed,
                subtasks: [],
                isExpanded: false,
                depth: 0,
                category: item.category || 'extracted'
              })),
              insights: {
                key_topics: result.insights?.key_topics || ['extracted items'],
                action_required: result.insights?.action_required || false,
                timeframe: result.insights?.timeframe || 'undetermined',
                summary: 'Points extracted from your note'
              }
            };
            notes.push(extractedTaskNote);
          }
        }
        
        console.log(`Created ${notes.length} notes from dictation`);
        setDictationNotes(notes);
        setShowDictationPreview(true);
        
        // Add debug message to track visibility
        console.log(`ðŸ”” DICTATION PREVIEW SHOULD NOW BE VISIBLE - Notes count: ${notes.length}`);
        
        // Add a small delay to ensure state is updated before continuing
        setTimeout(() => {
          document.body.style.overflow = 'hidden'; // Force body to not scroll
          const previewDiv = document.querySelector('.dictation-preview-panel');
          if (previewDiv) {
            (previewDiv as HTMLElement).style.display = 'block';
            (previewDiv as HTMLElement).style.opacity = '1';
            console.log('ðŸ” Found and forced dictation preview to be visible');
          } else {
            console.warn('âš ï¸ Could not find dictation preview element');
          }
        }, 100);
        
        setIsProcessing(false);
      } catch (error) {
        console.error('Error processing dictation with AI:', error);
        
        // Create a fallback note with error message and transcript
        createFallbackNote(transcript, "AI processing error");
      }
    } catch (error) {
      console.error("Error processing dictation with AI:", error);
      
      // Create a fallback note with error message and transcript
      createFallbackNote(transcript, "AI processing error");
    } finally {
      // Always show the dictation preview, even if there was an error
      console.log(`ðŸŽ¯ Transcript processing complete - dictation preview should now be visible`);
      setIsProcessing(false);
    }
  }
  
  // Helper function to create a fallback note when AI processing fails
  const createFallbackNote = (transcript: string, errorType: string) => {
    console.log(`Creating fallback note due to ${errorType}`);
    
    const notes: Note[] = [];
    
    // Create a note with the transcript
    const fallbackNote: Note = {
      id: `note-fallback-${Date.now()}`,
      type: 'sticky',
      color: 'yellow',
      content: `${transcript}\n\n(Created without AI processing due to ${errorType})`,
      position: { x: 100, y: 100 },
      size: { width: 250, height: 250 },
      insights: {
        key_topics: ['dictation', 'fallback'],
        action_required: false,
        timeframe: 'now',
        summary: 'Note created from dictation without AI processing'
      }
    };
    
    notes.push(fallbackNote);
    
    // Extract simple tasks if present (look for list markers)
    const lines = transcript.split('\n');
    const taskLines = lines.filter(line => 
      line.trim().startsWith('- ') || 
      line.trim().startsWith('â€¢ ') || 
      line.trim().startsWith('* ')
    );
    
    if (taskLines.length > 0) {
      const taskNote: Note = {
        id: `note-tasks-${Date.now()}`,
        type: 'task',
        color: 'blue',
        content: 'ðŸ“ Tasks extracted from dictation',
        position: { x: 350, y: 100 },
        size: { width: 250, height: 300 },
        tasks: taskLines.map((line, index) => ({
          id: `task-${Date.now()}-${index}`,
          text: line.replace(/^[-â€¢*]\s+/, '').trim(),
          done: false,
          subtasks: [],
          isExpanded: false,
          depth: 0,
          category: 'extracted'
        })),
        insights: {
          key_topics: ['tasks', 'extracted'],
          action_required: true,
          timeframe: 'soon',
          summary: 'Tasks extracted from dictation text'
        }
      };
      
      notes.push(taskNote);
    }
    
    console.log(`Created ${notes.length} fallback notes from dictation`);
    setDictationNotes(notes);
    setShowDictationPreview(true);
    
    // Add a small delay to ensure state is updated before continuing
    setTimeout(() => {
      document.body.style.overflow = 'hidden'; // Force body to not scroll
      const previewDiv = document.querySelector('.dictation-preview-panel');
      if (previewDiv) {
        (previewDiv as HTMLElement).style.display = 'block';
        (previewDiv as HTMLElement).style.opacity = '1';
        console.log('ðŸ” Found and forced dictation preview to be visible');
      } else {
        console.warn('âš ï¸ Could not find dictation preview element');
      }
    }, 100);
  };
  
  // Approve an individual note and add it to canvas
  const approveIndividualNote = (noteId: string) => {
    console.log(`Approving individual note: ${noteId}`);
    
    // Find the note in the dictation notes
    const noteToApprove = dictationNotes.find(note => note.id === noteId);
    if (!noteToApprove) {
      console.warn(`Note with ID ${noteId} not found in dictation notes`);
      return;
    }
    
    // Position this note in center of canvas
    const centerX = (window.innerWidth / 2) - 150;
    const centerY = (window.innerHeight / 2) - 100;
    
    // Create new note with proper ID and position
    const newNote = {
      ...noteToApprove,
      id: `note-${Date.now()}-${Math.random().toString(36).substring(2, 9)}`,
      position: {
        x: centerX,
        y: centerY
      }
    };
    
    // Add to notes collection
    setNotes(prevNotes => [...prevNotes, newNote]);
    
    // Remove this note from dictation notes
    setDictationNotes(prevNotes => prevNotes.filter(note => note.id !== noteId));
    
    // If no more notes left, close the preview
    if (dictationNotes.length <= 1) {
      setShowDictationPreview(false);
    }
    
    // Simulate agent activity
    simulateAgentActivity("success", `Added "${noteToApprove.title}" to your canvas.`);
  };

  // Update a note in the dictation preview
  const editDictationNote = (noteId: string, updates: Partial<any>) => {
    console.log(`Editing dictation note: ${noteId}`, updates);
    
    setDictationNotes(prevNotes => 
      prevNotes.map(note => 
        note.id === noteId 
          ? { ...note, ...updates } 
          : note
      )
    );
  };

  // Delete a note from the dictation preview
  const deleteDictationNote = (noteId: string) => {
    console.log(`Deleting dictation note: ${noteId}`);
    
    setDictationNotes(prevNotes => prevNotes.filter(note => note.id !== noteId));
    
    // If no more notes left, close the preview
    if (dictationNotes.length <= 1) {
      setShowDictationPreview(false);
      simulateAgentActivity("info", "Dictation notes removed.");
    }
  };

  // Update the approveDictation function to handle selected notes
  const approveDictation = (selectedNoteIds?: string[]) => {
    console.log(`Approving dictation notes: ${selectedNoteIds?.length || 'all'}`);
    
    if (!dictationNotes || dictationNotes.length === 0) {
      console.warn('No dictation notes to approve');
      setShowDictationPreview(false);
      return;
    }
    
    // Filter notes by selected IDs if provided
    const notesToApprove = selectedNoteIds
      ? dictationNotes.filter(note => selectedNoteIds.includes(note.id))
      : dictationNotes;
    
    // Position notes in center of canvas
    const centerX = (window.innerWidth / 2) - 150;
    const centerY = (window.innerHeight / 2) - 100;
    
    // Add each note with a slight offset
    const newNotes = notesToApprove.map((note, index) => {
      const newNote = {
        ...note,
        id: `note-${Date.now()}-${index}`,
        position: {
          x: centerX + (index * 50),
          y: centerY + (index * 50)
        }
      };
      return newNote;
    });
    
    // Update notes state
    setNotes(prevNotes => [...prevNotes, ...newNotes]);
    console.log(`Added ${newNotes.length} new notes from dictation`);
    
    // Clear dictation preview
    setDictationNotes([]);
    setShowDictationPreview(false);
    
    // Simulate agent activity
    simulateAgentActivity("success", `Created ${newNotes.length} new note${newNotes.length > 1 ? 's' : ''} from your dictation.`);
  };

  // Cancel dictation and discard notes
  const cancelDictation = () => {
    console.log('Canceling dictation');
    setDictationNotes([]);
    setShowDictationPreview(false);
    
    // Simulate agent activity
    simulateAgentActivity("info", "Dictation notes discarded.");
  };
  
  // Command execution handler
  const handleExecuteCommand = (command: string) => {
    closeCommandPalette();
    
    if (command === 'add-note') {
      addNote();
    } else if (command === 'clear-all') {
      setNotes([]);
    } else if (command === 'toggle-mini-map') {
      toggleMiniMap();
    } else if (command === 'reset-view') {
      setCanvasOffset({ x: 0, y: 0 });
      setScale(1);
    } else if (command === 'create-notes-from-text') {
      simulateAgentActivity("generating", "Creating notes from text...", 3000);
    } else if (command === 'web-search') {
      simulateAgentActivity("searching", "Searching the web for information...", 4000);
    } else if (command === 'summarize') {
      simulateAgentActivity("processing", "Summarizing content...", 5000);
    } else if (command === 'generate-tasks') {
      simulateAgentActivity("generating", "Generating tasks from your notes...", 3500);
    }
  };
  
  useEffect(() => {
    // Hide movement tooltip after 10 seconds
    const timer = setTimeout(() => {
      setShowMovementTooltip(false);
    }, 10000);
    
    return () => clearTimeout(timer);
  }, []);

  const handleKeyDown = (e: KeyboardEvent) => {
    // Command palette shortcut
    if ((e.ctrlKey || e.metaKey) && e.key === 'k') {
      e.preventDefault();
      openCommandPalette();
    }
  };

  const handleKeyUp = (e: KeyboardEvent) => {
    // No need to handle space key up anymore
  };

  const startDrag = (e: React.MouseEvent<HTMLDivElement>) => {
    // Only start canvas drag if we click directly on the canvas background (not on a note)
    if (!clickedOnNote) {
      setIsDraggingCanvas(true);
      setIsDragging(true);
      setStartPoint({ x: e.clientX, y: e.clientY });
      document.body.style.cursor = 'grabbing';
      e.preventDefault(); // Prevent other events
    }
  };

  const onDrag = (e: React.MouseEvent<HTMLDivElement>) => {
    if (isDragging && isDraggingCanvas) {
      const dx = e.clientX - startPoint.x;
      const dy = e.clientY - startPoint.y;
      setCanvasOffset(prev => ({
        x: prev.x + dx,
        y: prev.y + dy
      }));
      setStartPoint({ x: e.clientX, y: e.clientY });
    }
    
    // Update last mouse position for the mini-map viewport indicator
    setLastMousePosition({ x: e.clientX, y: e.clientY });
  };

  const endDrag = () => {
    if (isDragging) {
      setIsDragging(false);
      setIsDraggingCanvas(false);
      document.body.style.cursor = 'default';
      // Reset clicked on note state
      setClickedOnNote(false);
    }
  };

  // Track when user clicks on a note to prevent canvas dragging
  const handleNoteMouseDown = (id: string) => {
    setClickedOnNote(true);
    setSelectedNoteId(id);
  };

  const handleWheel = (e: React.WheelEvent<HTMLDivElement>) => {
    // Check if the Ctrl key (or Command key on Mac) is pressed for zoom
    const isZoomEvent = e.ctrlKey || e.metaKey;
    
    if (isZoomEvent) {
      e.preventDefault(); // Prevent default scrolling behavior
      
      // Determine zoom direction
      const zoomFactor = e.deltaY > 0 ? 0.9 : 1.1; // Zoom out (0.9) or in (1.1)
      
      // Calculate new scale with limits
      const newScale = Math.max(0.2, Math.min(5, scale * zoomFactor));
      
      // Get the mouse position relative to the canvas
      const rect = viewportRef.current?.getBoundingClientRect();
      if (!rect) return;
      
      const mouseX = e.clientX - rect.left;
      const mouseY = e.clientY - rect.top;
      
      // Calculate mouse position in canvas coordinates
      const mouseXInCanvas = mouseX / scale - canvasOffset.x;
      const mouseYInCanvas = mouseY / scale - canvasOffset.y;
      
      // Calculate new offsets to zoom toward/from the mouse position
      const newOffsetX = mouseX / newScale - mouseXInCanvas;
      const newOffsetY = mouseY / newScale - mouseYInCanvas;
      
      // Update state
      setScale(newScale);
      setCanvasOffset({ x: newOffsetX, y: newOffsetY });
    } else {
      // For regular scrolling (panning the canvas)
      setCanvasOffset({
        x: canvasOffset.x - e.deltaX / scale,
        y: canvasOffset.y - e.deltaY / scale
      });
    }
  };

  useEffect(() => {
    document.addEventListener('keydown', handleKeyDown);
    document.addEventListener('keyup', handleKeyUp);
    
    const handleResize = () => {
      if (viewportRef.current) {
        setViewportSize({
          width: viewportRef.current.clientWidth,
          height: viewportRef.current.clientHeight
        });
      }
    };
    
    window.addEventListener('resize', handleResize);
    // Initial size
    handleResize();
    
    return () => {
      document.removeEventListener('keydown', handleKeyDown);
      document.removeEventListener('keyup', handleKeyUp);
      window.removeEventListener('resize', handleResize);
    };
  }, []);

  const AppNavigation = () => {
    const navigate = useNavigate();
    
    return (
      <div className="app-navbar bg-gray-900 border-b border-gray-800">
        <div className="navbar-brand">
          <Link to="/" className="navbar-logo">
            <h1>VibeFlo</h1>
          </Link>
        </div>
        
        <div className="nav-actions">
          {/* Add Notes toggle button */}
          <button onClick={toggleNoteSidebar} title="Notes Explorer" className={`nav-notes-button ${showNoteSidebar ? 'active' : ''}`}>
            <FileStack size={18} />
            <span>Notes</span>
          </button>
          
          <button onClick={() => navigate('/')} title="Home - Freeform Canvas">
            <Home size={18} />
            <span>Canvas</span>
          </button>
          <button onClick={addNote} title="Add Note">
            <Grid size={18} />
            <span>Add Note</span>
          </button>
          
          {/* Add Agent Sidebar toggle button */}
          <button 
            onClick={toggleAgentSidebar} 
            title="AI Agent Capabilities"
            className={`nav-agent-button ${showAgentSidebar ? 'active' : ''}`}
          >
            <Zap size={18} />
            <span>Agent</span>
          </button>
          
          {/* Add Command Palette button */}
          <button 
            onClick={openCommandPalette} 
            title="Command Palette (Ctrl+K)"
            className="nav-command-button"
          >
            <Search size={18} />
            <span>Commands</span>
          </button>
          
          <div className="ml-auto flex items-center">
            <button
              className={`app-nav-button voice-button ${recording ? 'recording' : ''}`}
              onClick={toggleRecording}
              title={recording ? "Stop Recording" : "Start Voice Dictation"}
            >
              <Mic className="w-5 h-5" />
              <span className="text-sm">
                {recording ? 'Recording...' : 'Voice Input'}
              </span>
              {recording && (
                <div className="recording-indicator">
                  <div className="recording-dot"></div>
                </div>
              )}
            </button>
          </div>
        </div>
      </div>
    );
  };

  // Fix the calculateTotalNutrition function with proper type annotations
  const calculateTotalNutrition = (tasks: Array<any>): NutritionData => {
    return tasks.reduce((totals: NutritionData, task: any) => {
      if (task.nutrition) {
        return {
          calories: totals.calories + (task.nutrition.calories || 0),
          protein: totals.protein + (task.nutrition.protein || 0),
          carbs: totals.carbs + (task.nutrition.carbs || 0),
          fat: totals.fat + (task.nutrition.fat || 0),
        };
      }
      return totals;
    }, { calories: 0, protein: 0, carbs: 0, fat: 0 });

  // Since there are too many type conflicts, let's modify our approach
  // Instead of completely changing the Note type, we'll just use any 
  // for the returned object in createNoteFromData to avoid conflicts
  // and then cast it appropriately when needed.
  const createNoteFromData = (data: any, yOffset: number): any => {
    if (!data) return null;
    
    const basePosition = { x: 100, y: 100 + yOffset };
    
    if (data.type === 'grocery_list') {
      // Process grocery items and add nutrition info
      const tasks = Array.isArray(data.items) ? data.items.map((item: any) => {
        // Look for nutrition data for this item
        const itemText = item.text.toLowerCase();
        let nutrition = null;
        
        // Search for known food items in the text
        for (const [food, values] of Object.entries(NUTRITION_DATABASE)) {
          if (itemText.includes(food)) {
            nutrition = values;
            break;
          }
        }
        
        return {
          id: `task-${Date.now()}-${Math.random().toString(36).substring(2, 9)}`,
          text: item.text,
          done: item.completed || false,
          subtasks: [],
          isExpanded: false,
          depth: 0,
          category: item.category || 'grocery',
          quantity: item.quantity || '',
          nutrition: nutrition
        };
      }) : [];
      
      // Calculate total nutrition
      const totalNutrition = calculateTotalNutrition(tasks);
      
      // Create grocery note with nutrition data
      return {
        id: `note-${Date.now()}-${Math.random().toString(36).substring(2, 9)}`,
        type: 'task',
        color: 'green',
        content: `ðŸ›’ ${data.title}\n\nGrocery List:`,
        position: basePosition,
        size: { width: 280, height: 350 },
        tasks: tasks,
        insights: {
          key_topics: ['groceries', 'shopping', 'food'],
          action_required: true,
          timeframe: 'soon',
          nutrition_summary: totalNutrition
        }
      };
    } else if (data.type === 'task_list') {
      return {
        id: `note-${Date.now()}-${Math.random().toString(36).substring(2, 9)}`,
        type: 'task',
        color: 'blue',
        content: `ðŸ“ ${data.title}\n\nâœ… Task List:`,
        position: basePosition,
        size: { width: 280, height: 350 },
        tasks: Array.isArray(data.items) ? data.items.map((item: any) => ({
          id: `task-${Date.now()}-${Math.random().toString(36).substring(2, 9)}`,
          text: item.text,
          done: item.completed || false,
          subtasks: [],
          isExpanded: false,
          depth: 0,
          category: item.category || 'task'
        })) : [],
        insights: data.insights || {
          key_topics: ['tasks', 'todos', 'productivity'],
          action_required: true,
          timeframe: 'upcoming'
        }
      };
    } else if (data.type === 'recipe') {
      return {
        id: `note-${Date.now()}-${Math.random().toString(36).substring(2, 9)}`,
        type: 'sticky',
        color: 'pink',
        content: `ðŸ½ï¸ ${data.title}\n\n${data.content || ''}`,
        position: basePosition,
        size: { width: 280, height: 350 },
        items: Array.isArray(data.items) ? data.items.map((item: any) => ({
          id: `ingredient-${Date.now()}-${Math.random().toString(36).substring(2, 9)}`,
          text: item.text,
          completed: false,
          category: 'ingredient',
          quantity: item.quantity || ''
        })) : [],
        insights: data.insights || {
          key_topics: ['recipe', 'cooking', 'food'],
          action_required: false,
          timeframe: 'anytime',
          summary: `A recipe for ${data.title} with ${Array.isArray(data.items) ? data.items.length : 0} ingredients.`
        }
      };
    } else {
      // Default to general note
      return {
        id: `note-${Date.now()}-${Math.random().toString(36).substring(2, 9)}`,
        type: 'sticky',
        color: 'yellow',
        content: `${data.title}\n\n${data.content || ''}`,
        position: basePosition,
        size: { width: 280, height: 300 },
        insights: data.insights || {
          key_topics: data.key_topics || ['note', 'information'],
          summary: `General note: ${data.title}`
        }
      };
    }
  };

  return (
    <BrowserRouter>
      <DndProvider backend={HTML5Backend}>
        <div className="app">
          {/* Centered Recording Tooltip */}
          <div className={`recording-tooltip ${recording ? 'recording-active' : ''}`}>
            <button 
              className="recording-tooltip-close" 
              onClick={() => recording ? toggleRecording() : null}
            >
              <X size={20} />
            </button>
            <div className="recording-tooltip-content">
              {recording ? 
                <>
                  <div className="recording-status">
                    <div className="recording-pulse"></div>
                    <span>Recording... {recordingTime > 0 ? new Date(recordingTime * 1000).toISOString().substr(14, 5) : ''}</span>
                  </div>
                  <button 
                    className="recording-stop-button"
                    onClick={toggleRecording}
                  >
                    Stop Recording
                  </button>
                </>
                : 
                <div className="recording-instructions">
                  Press to start recording
                </div>
              }
            </div>
          </div>

          <AppNavigation />
          <div className="app-content">
            <Routes>
              <Route path="/" element={
                <div 
                  ref={viewportRef}
                  className="canvas-viewport" 
                  onMouseDown={startDrag}
                  onMouseMove={onDrag}
                  onMouseUp={endDrag}
                  onMouseLeave={endDrag}
                  onWheel={handleWheel}
                >
                  {/* Canvas Movement Tooltip */}
                  {showMovementTooltip && (
                    <div className="canvas-movement-tooltip">
                      <div className="tooltip-header">
                        <HelpCircle size={18} />
                        <span>Canvas Movement Controls</span>
                        <button 
                          className="dismiss-tooltip"
                          onClick={() => setShowMovementTooltip(false)}
                        >
                          <X size={16} />
                        </button>
                      </div>
                      <div className="tooltip-content">
                        <p><strong>Hold SPACE + Drag</strong> to move the canvas</p>
                        <p><strong>CTRL + Scroll</strong> to zoom in/out</p>
                        <p>Use the zoom controls in the bottom right corner</p>
                      </div>
                    </div>
                  )}
                  
                  <div
                    ref={canvasRef}
                    className={`canvas ${isDragging ? 'dragging' : ''}`}
                    style={{
                      transform: `translate(${canvasOffset.x}px, ${canvasOffset.y}px) scale(${scale})`,
                      width: canvasSize.width,
                      height: canvasSize.height,
                      cursor: isDraggingCanvas ? 'grabbing' : 'default'
                    }}
                  >
                    {notes.map(note => (
                      <NoteComponent
                        key={note.id}
                        note={note}
                        onMove={(id, newPosition) => handleMove(id, newPosition)}
                        onContentChange={(id, content) => updateNote(id, { content })}
                        isSelected={selectedNoteId === note.id}
                        onSelect={(id) => setSelectedNoteId(id === selectedNoteId ? null : id)}
                        onStartDrag={(id) => {
                          setClickedOnNote(true);
                          setSelectedNoteId(id);
                        }}
                      />
                    ))}
                  </div>
                  
                  {/* Zoom controls */}
                  <div className="zoom-controls">
                    <button 
                      onClick={() => setScale(prev => Math.max(prev - 0.1, 0.1))}
                      className="zoom-button"
                      title="Zoom Out"
                    >
                      <ChevronLeft size={20} />
                    </button>
                    <div className="zoom-display">
                      {Math.round(scale * 100)}%
                    </div>
                    <button 
                      onClick={() => setScale(prev => Math.min(prev + 0.1, 5))}
                      className="zoom-button"
                      title="Zoom In"
                    >
                      <ChevronRight size={20} />
                    </button>
                  </div>
                  
                  {/* Mini-map */}
                  {showMiniMap && (
                    <div className="mini-map-container">
                      <div className="mini-map-header">
                        <span>Canvas Overview</span>
                        <button onClick={toggleMiniMap} className="mini-map-close">
                          <X size={16} />
                        </button>
                      </div>
                      <div className="mini-map">
                        {notes.map(note => (
                          <div 
                            key={`map-${note.id}`}
                            className={`mini-map-note ${note.color}`}
                            style={{
                              left: `${(note.position.x / canvasSize.width) * 100}%`,
                              top: `${(note.position.y / canvasSize.height) * 100}%`
                            }}
                            title={note.content.substring(0, 20)}
                          />
                        ))}
                        
                        {/* Viewport indicator */}
                        <div 
                          className="mini-map-viewport"
                          style={{
                            width: `${(viewportSize.width / canvasSize.width) * 100}%`,
                            height: `${(viewportSize.height / canvasSize.height) * 100}%`,
                            left: `${(-canvasOffset.x / canvasSize.width) * 100}%`,
                            top: `${(-canvasOffset.y / canvasSize.height) * 100}%`
                          }}
                        />
                      </div>
                    </div>
                  )}
                </div>
              } />
            </Routes>
          </div>
          
          {/* Floating microphone button for mobile/tablet */}
          <div className={`floating-mic ${isRecording ? 'recording' : ''}`}>
            <button 
              className="floating-mic-button" 
              onClick={toggleRecording}
              aria-label={isRecording ? "Stop recording" : "Start voice recording"}
            >
              <Mic size={24} />
            </button>
            {isRecording && (
              <div className="recording-duration">
                {formatRecordingTime(recordingTime)}
              </div>
            )}
          </div>

          {/* Real-time Dictation Popup */}
          {showDictationPopup && (
            <div className="dictation-preview-panel recording-panel">
              <div className="dictation-preview-header">
                <h3>Voice Recording</h3>
                <div className="dictation-preview-actions">
                  <button 
                    className="dictation-preview-action cancel" 
                    onClick={toggleRecording}
                  >
                    Stop Recording
                  </button>
                </div>
              </div>

              <div className="dictation-preview-content">
                <div className={`recording-indicator ${isProcessing ? 'warning' : ''}`}>
                  <div className="recording-pulse"></div>
                  <span>
                    {isProcessing ? 'Processing...' : `Recording ${formatRecordingTime(recordingTime)}`}
                    {visibleTranscript && visibleTranscript !== 'Listening to your voice...' && 
                     !isProcessing && ` â€¢ Speech detected`}
                  </span>
                </div>
                
                <div className="dictation-transcript">
                  {visibleTranscript && visibleTranscript !== 'Listening to your voice...' ? (
                    <p>
                      {visibleTranscript.replace(/\[(.*?)\]$/, '<span>$1</span>').split('<span>').map((part, index, array) => {
                        // If this is the last part and we have more than one part, it's the interim text
                        if (index === array.length - 1 && array.length > 1 && part) {
                          return <span key={index} className="interim-text">[{part}</span>;
                        }
                        return part;
                      })}
                    </p>
                  ) : (
                    <p className="transcript-placeholder">
                      {isProcessing ? 'Processing your speech...' : 'Speak now...'}
                    </p>
                  )}
                </div>
                
                <div style={{ marginTop: '12px', fontSize: '13px', color: 'rgba(255,255,255,0.5)', textAlign: 'center' }}>
                  {!isProcessing && 'Speak clearly into your microphone. Click Stop when finished.'}
                </div>
              </div>
            </div>
          )}

          {/* Dictation Preview Panel (after processing) */}
          {showDictationPreview && (
            <DictationPreview
              isVisible={showDictationPreview}
              transcript={transcript}
              interimTranscript={interimTranscript}
              notes={dictationNotes}
              onApprove={approveDictation}
              onCancel={cancelDictation}
              isRecording={recording}
              onApproveIndividual={approveIndividualNote}
              onEdit={editDictationNote}
              onDelete={deleteDictationNote}
            />
          )}

          <AgentSidebar isOpen={showAgentSidebar} onClose={toggleAgentSidebar} />
          <CommandPalette 
            isOpen={showCommandPalette} 
            onClose={closeCommandPalette} 
            onExecuteCommand={handleExecuteCommand} 
          />
          <AgentFeedback 
            isVisible={showAgentFeedback}
            activity={agentActivity}
            message={agentMessage}
            progress={agentProgress}
            confidence={agentConfidence}
            onClose={() => setShowAgentFeedback(false)}
          />

          {/* Note Sidebar */}
          <NoteSidebar 
            notes={notes}
            isOpen={showNoteSidebar}
            onClose={toggleNoteSidebar}
            onUpdateNote={updateNote}
            onSelectNote={(id) => setSelectedNoteId(id)}
            onFocusNote={focusOnNote}
          />
        </div>
      </DndProvider>
    </BrowserRouter>
  );
}

// Add type definitions for the Web Speech API
declare global {
  interface Window {
    SpeechRecognition: any;
    webkitSpeechRecognition: any;
    speechRecognitionInstance: any;
  }
  
  interface SpeechRecognitionEvent extends Event {
    resultIndex: number;
    results: {
      [index: number]: {
        [index: number]: {
          transcript: string;
        };
        isFinal: boolean;
        length: number;
      };
      length: number;
    };
  }
  
  interface SpeechRecognitionErrorEvent extends Event {
    error: string;
  }
}

export default App;